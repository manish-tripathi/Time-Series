---
title: "Forcasting Analysis Individual Assignment"
author: "Manish Tripathi"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Importing the libraries

library(readxl)
library(ggpubr)
library(forecast)
```


```{r}
# Importing the dataset and converting it into a time series

SouvenirSales <- read_excel("C:/Users/PankhuriManish/Desktop/FA/SouvenirSales.xlsx",
                            col_types = c("date", "numeric"))

SouvenirSales.ts <- ts(SouvenirSales$Sales, start = c(1995,1), frequency = 12)
SouvenirSales.ts
```

## a. Plot the time series of the original data. Which time series components appear from the plot.

```{r}
# Visualizing the data

autoplot(SouvenirSales.ts, color = "blue") +   ylab("Sales") +
  xlab("Year") + ggtitle("Souvenir Sales") +
  theme(plot.title = element_text(hjust = 0.1)) + 
  scale_x_continuous(breaks = seq(1995, 2002))

```


Based on the time plot the Souvenir Sales data seems to have the following components:

i) Level: All time series data have level by default
ii) Trend: There seems to be an increasing trend
iii) Seasonality: the observations towards the end of the year show a repeatitive pattern with sparp spike, suggesting presence of seasonality.

Also the seasonality component seems to be increasing by some factor so seems like a multiplicative time series with trend and seasonality.

To better understand the components present in the time series, decomposing the time series using both Additive and Multiplicative Decomposition methods.

```{r}
# Additive Decomposition of the time series

ss1 <- decompose(SouvenirSales.ts, type= "additive")
autoplot(ss1)


# Multiplicative Decomposition of the time series
ss2 <- decompose(SouvenirSales.ts, type= "multiplicative")
autoplot(ss2)


```

To see which decomposition fits the data better, we will calculate the Root mean squared errors of both decompositions.

```{r}
# Estimating RMSE of Additivite Decomposition
sqrt(mean(na.omit(ss1$random)^2)) 
# Estimating RMSE of Multiplicative Decomposition
sqrt(mean(na.omit(ss2$random)^2))
```

based on the RMSE, Multiplicative Decomposition of data seems to give better results.

## b. Fit a linear trend model with additive seasonality (Model A) and exponential trend model with multiplicative seasonality (Model B). Consider January as the reference group for each model. Produce the regression coefficients and the validation set errors. Remember to fit only the training period.

```{r}
# Splitting the data into train and test

train <- window(SouvenirSales.ts,end=c(2000,12), frequency=12)
test <- window(SouvenirSales.ts,start=c(2001,1), frequency=12)
```



```{r}

# Building Linear Trend with Additive Seasonality
ModelA <- tslm(train ~ trend + season) 
summary(ModelA)
ModelA
```


Predictions for Model A

```{r}
ModelA.pred <- forecast(ModelA, h=length(test), level =0)
ModelA.pred
```


Errors for Linear Trend with Additive Seasonality (Model A)

```{r}
accuracy(ModelA.pred$mean,test)
```

Building Exponential Trend with Multiplicative Seasonality

```{r}
# Building Exponential Trend with Multiplicative Seasonality
ModelB <- tslm(train ~ trend + season, lambda = 0) 
summary(ModelB)
ModelB
```

Predictions for Model B

```{r}
ModelB.pred <- forecast(ModelB, h=length(test), level =0)
ModelB.pred
```


Errors for Exponential Trend with Multiplicative Seasonality (Model B)

```{r}
accuracy(ModelB.pred$mean,test)
```


## c. Which model is the best model considering RMSE as the metric? Could you have understood this from the line chart? Explain. Produce the plot showing the forecasts from both models along with actual data. In a separate plot, present the residuals from both models (consider only the validation set residuals).

Model B i.e. Exponential Trend with Multiplicative Seasonality is a better model considering the RMSE.

Based on solely the line chart also we could see a multiplicative effect for seasonality as the magnitude of susequent spike was much greate than the previous spike, but could not have commented on the exponential trend.

Plot showing the forecast for Linear Trend with Additive Seasonality (Model A)

```{r}
plot(ModelA.pred , xlab ="Time", ylab= "Sales", 
     main="Linear Trend with \n Additive Seasonality (Model A)",
     flty=1,  bty="l",ylim= c(0,120000))

lines(ModelA.pred$fitted,lwd=2)
lines(test,col="blue")
abline(v=2001, col="red", lwd=2)
```

Plot showing the forecast for Exponential Trend with Multiplicative Seasonality (Model B)

```{r}
plot(ModelB.pred,  xlab ="Time", ylab= "Sales", 
     main="Exponential Trend with \n Multiplicative Seasonality (Model B)",
     flty=1,ylim=c(0,120000), bty="l" ) 
lines(ModelB.pred$fitted,lwd=2)
lines(test,col="blue")
lines(train)
abline(v=2001, col="red", lwd=2)
```

Based on the two plots we can see that the Model B is better at predicting the data.

Residuals from Model A (Both Train and Test)

```{r}
plot(ModelA$residuals, ylim= c(-60000,60000),
     main= "Residual Plot Model A", ylab="Forecast Error", 
     xlim= c(1995, 2003), xaxp = c(1995, 2002, 2002-1995))
lines(test-ModelA.pred$mean, col="blue")
abline(v=2001, col="red", lwd=2)
text(1998,1, "Training",pos = 3)
text(2002,1, "Validation",pos= 1)
```

Residuals from Model B (Both Train and Test)

```{r}
plot(ModelB$residuals, ylim= c(-60000,60000),
     main= "Residual Plot Model B", ylab="Forecast Error", 
     xlim= c(1995, 2003), xaxp = c(1995, 2002, 2002-1995))
lines(test-ModelB.pred$mean, col="blue")
abline(v=2001, col="red", lwd=2)
text(1998,1, "Training",pos = 3)
text(2002,12, "Validation",pos= 1)
```



Residuals from Model A (Only Test/Validation Set)

```{r}
plot(test-ModelA.pred$mean, ylim= c(-10000,50000),
     main= "Residual Plot Model A \n Only Validation Set",
     ylab="Forecast Error")

```


Residuals from Model B (Only Test/Validation Set)

```{r}
plot(test-ModelB.pred$mean, ylim= c(-10000,50000),
     main= "Residual Plot Model B \n Validation Set Only", 
ylab="Forecast Error")
```


## d. Examine the additive model. Which month has the highest average sales during the year. What does the estimated trend coefficient in the model A mean?

December has the highest average sales during the year. The estimated trend coefficient in the model A means that for each unit increase in month, the sales increase by an amount of USD 245.4.

## e. Examine the multiplicative model. What does the coefficient of October mean? What does the estimated trend coefficient in the model B mean?

The coefficient of October means that sales in October of any year are 72.9% higher than the sales in January of that particular year, as the base month here is January. The estimated trend coefficient in the model B means that for each unit increase in month, the sales increase by 2.1%. 


## f. Use the best model type from part (c) to forecast the sales in January 2002. Think carefully which data to use for model fitting in this case.

As the RMSE for Exponential Trend with Multiplicative Seasonality (Model B) is lesser than that of Linear Trend with Additive Seasonality (Model A). We will select Model B for Prediction. As we have selected our model we will retrain the model on the entire dataset.

```{r}
# Building Exponential Trend with Multiplicative Seasonality
ModelB.retrained <- tslm(SouvenirSales.ts ~ trend + season, lambda = 0) 
summary(ModelB.retrained)
ModelB.retrained
```


Forcasting for January 2002

```{r}
ModelB.retrained.pred <- forecast(ModelB.retrained, h=1, level =95)
ModelB.retrained.pred
```

## g. Plot the ACF and PACF plot until lag 20 of the residuals obtained from training set of the best model chosen. Comment on these plots and think what AR(p) model could be a good choice?

ACF Plot
```{r}
Acf(ModelB.retrained$residuals,lag.max = 20)
```

PACF Plot
```{r}
Pacf(ModelB.retrained$residuals,lag.max = 20)
```

Based on the ACF and the PACF plots the AR (2) model could be a good choice as the ACF plot has very significant lag 1 and lag 2 and significant lag 3 bar. Also we have a decreasing pattern which is sinosuidal.

Even The PACF plot has very significant lag 1 and lag 2 significantly outside the white noise boundary, then the rest of lags are insignificant.

THe ACF and the PACF plots together suggest a AR(2) model.

## h. Fit an AR(p) model as you think appropriate from part (h) to the training set residuals and produce the regression coefficients. Was your intuition at part (h) correct?

```{r}
errors = ModelB.retrained$residuals
ModelB.retrained.res.arima <- Arima(errors, order = c(2,0,0))
summary(ModelB.retrained.res.arima)
Acf(ModelB.retrained.res.arima$residuals, lag.max=20)
Pacf(ModelB.retrained.res.arima$residuals, lag.max=20)
```

Following the AR(2) model the auto correlations at lag 1 and lag 2 have become insignificant. The intution seems to be correct.

## i. Now, using the best regression model and AR(p) model, forecast the sales in January 2002. Think carefully which data to use for model fitting in this case.


```{r}

ModelB.retrained.res.arima.pred <- forecast(ModelB.retrained.res.arima, h=1, level =95)
summary(ModelB.retrained.res.arima.pred)
```

Forcast for January 2002 based on Model B and AR(2) model will be:

```{r}
Forecast_Jan2002 = ModelB.retrained.res.arima.pred$mean +
  ModelB.retrained.pred$mean
Forecast_Jan2002
```

## 2. Short answer type questions:

## a. Explain the key difference between cross sectional and time series data.

`Cross Sectional Data:` Observations of data collected at a given point of time. For example: a) Name of Employees, Salary credited, Tax Deducted in the month of May, b) Marks obtained by students of a particular school in the Class XII boards. The underlying assumption about the data is that it is Independently and Identically Distributed or Randomly Distributed.

`Time Series Data:` Observations are recorded over a period of time, for example, rainfall recorded over  past 10 years, monthly number of tourists who visited India in past 12 months. The data in time series shows auto correlation or seriel correlation where the data in time "t" may be correlated to data from time "t-1" or previous time periods. A time series data is comprised of one or more of the components described below:

i) Level
ii) Trend
iii) Sesonality
iv) Cyclicality, and 
v) Noise 

## b. Explain the difference between seasonality and cyclicality. 

`Seasonality:` is a short term variation in time series data due to seasonal factors. The distances between the two seasonal cycles should be equal i.e the up and down pattern should repeat at regular intervals. It can be caused due to seasonal factors during certain times in year, month, week, day or hour. For example, Heavy rush of customers in mall during weekends or during certain days of year such a Christmas or New Year. There can be multiple seasonal cycles can coexist in the time series data.

There are two types of seasonality:

`i) Additive Seasonality:` is when the the values increase or decrease by a constant amount 

`ii) Multiplicative Sesonality:` is when the values change by a constant degree


`Cyclicality:`Irregular pattrens in the time series data with medium term repetition. For example, The GDP data of country for past 200 years will show impacvt of multiple recessions but the recessions do not repeat after same number of years. This is Cyclicality.


## c. Explain why centered moving average is not-considered suitable for forecasting.

In the centered moving average, the trend line will loose some observations at the beginning and some at the end, while in the trailing moving average all the observations lost are at the beginning. The forcasting horizon will be larger if we use the centered moving average as compared to the trailing moving average. For example if the window size is 5 and we want to forcast one period ahead, the forcasting horizon with centered moving average will be 3, whereas with trailing moving average it will be 1. We will always prefer a shorter forcasting horizon as that reduces the chances of error in forcast.

## d. Explain stationarity and why is it important for some time series forecasting methods?

A time series data is called stationary if its mean, varience and covarience do not change with time. A time series with trend or seasonality is not stationary as with trend or sesanality the mean and varience may increase or decrease over a period of time. A time series which only has noise is stationary, as the observations in such a time series is randomly distributed.


If the mean, varience or covarience change over time it becomes difficult to forcast future values. Assumption of stationarity implies data is not dependant on time, for example if the sample mean and varience decrease over time we will always be over forcasting based on current values of mean and varience or vice versa. Also most forcasting models work on the assumption that the time series data is stationary. Hence making stationarity important for forcasting mentods.


## e. How does an ACF plot help to identify whether a time series is stationary or not? 

If the data is not stationary then the ACF plot drops to zero slowly. ACF plot for stationary data drops to zero rapidly. Also if the data is not stationary, the r1 value of the ACF plot will be usually large and positive.


## f. Why partitioning time series data into training, validation, and test set is not recommended? Briefly describe two considerations for choosing the width of validation period.

In time series data the most recent observations are the most relavant and most informative as they tell us most about the current scenario, if we divide our data into train, validation and test set, we will be training our model on fairly old data, the scenario could have changed quite a lot in present. Also our forcasting horizon will be fairly large. So in time series data, instead of splitting our data into train, validation and test set, we split our data into train and validation set only. We train our model on the training set and use the validation set for model selection. Once the model is selected, we train our model again on the entire time series data to estimate the parameters of the model used for forcasting. This way we are able incorporate the effect of most recent data in our model. 

The width of validation period depends upon:

`Forcasting Horizon:` The validation set width should be similar to the forcasing horizon, for example, if we are looking to forecast next twelve months of sales, the validation set period should be equal to 12, else the the forcasting horizon will fail to mimic the actual scenario. Also if the  validation set width is longer, recent information will not be incorporated in our training set and our model will be deficient.

`Seasonality:` The validation set should also be equal to the seasonal cycle, else we will fail to see if the model is corretly forecasting the seasonal variations. If their mare multiple seasonal cycles, the width should be so selected so as to incorporate all the seasonal cycles. 

Other things that determine the width of validation period are Forecasting goal, Data frequency and Length of the series.


## g. Both smoothing and ARIMA method of forecasting can handle time series data with missing value. True/False. Explain 

False. Both soothening and ARIMA can not be used if certain observations in the time series are missing. Both the methods see if there is a correlation between the present value and the past value to forcast. If the to be forcasted value is dependant of the missing value then we will not have any forcast. In such a scenario, we either impute the missing value or use models like linear or logistic regression. Kalman Filter is used as one of the ways to impute the missing values in time series data.


## h. Additive and multiplicative decomposition differ in the way the trend is computed. True /False. Explain. 

False. The trend is calculated the same way for both additive as well as multiplicative decompositions. Moving average with appropriate window is used to compute the trend. The moving average helps supress sesonality and noise and leaves us with trend.

However, the detrending of series is done differently in Additive and multiplicative decomposition. While in Additive decomposition the moving average is subtracted from the observations to get the detrended series. In multiplicative decomposition the observations are divided by the moving average to get the detrended series.


## i. After accounting for trend and seasonality in a time series data, the analyst observes that there is still correlation left amongst the residuals of the time series. Is that a good or a bad news for the analyst? Explain.

This can be considered as good news for the analyst. If there was no correlation in the residuals, there is nothing more the analyst can do with the residuals as they will be completely random, but on the other hand if there is still correlation left in the residuals, the analyst can derieve further information from the residuals and improve the model. If the data is not autocorrelated, we can not improve the model beyond a naive forcast.
